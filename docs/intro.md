---
sidebar_position: 1
---

# 关于 DSPy

**DSPy 是一个用于算法优化 LM 提示和权重的框架**，特别是当 LM 在管道中使用一次或多次时。要在不使用 DSPy 的情况下使用 LM 来构建复杂系统，通常需要：(1) 将问题分解为步骤，(2) 有效地提示您的 LM，直到每个步骤在隔离状态下运行良好，(3) 调整步骤以使其共同运行良好，(4) 生成合成示例来调整每个步骤，以及 (5) 使用这些示例来微调较小的 LM 以降低成本。目前，这很困难且混乱：每次更改管道、LM 或数据时，所有提示 (或微调步骤) 可能都需要更改。

为了使这更加系统化和更加强大，**DSPy** 做了两件事。首先，它将程序的流程 (`modules`) 与每个步骤的参数 (LM 提示和权重) 分开。其次，**DSPy** 引入了新的 `optimizers`，这些是可以调整 LM 调用的提示和/或权重的 LM 驱动算法，给定您想要最大化的 `metric`。

**DSPy** 可以经常教授像 `GPT-3.5` 或 `GPT-4` 这样的强大模型以及像 `T5-base` 或 `Llama2-13b` 这样的本地模型更加可靠地执行任务，即具有更高的质量和/或避免特定的失败模式。**DSPy** 优化器将将相同的程序编译成不同的指令、few-shot 提示和/或权重更新 (微调) 用于每个 LM。这是一个新的范式，其中 LM 及其提示淡化为一个更大系统中可从数据中学习的可优化部分。**tldr;** 更少的提示，更高的分数，以及更系统化的方法来使用 LM 解决困难任务。

## 与神经网络的类比

当我们构建神经网络时，我们不会手动编写对列表中手动调整的浮点数的 _for-loops_。相反，您可能会使用类似 [PyTorch](https://pytorch.org/) 的框架来组合层 (例如 `Convolution` 或 `Dropout`)，然后使用优化器 (例如 SGD 或 Adam) 来学习网络的参数。

同样！**DSPy** 为您提供了正确的通用模块 (例如 `ChainOfThought`, `ReAct` 等)，这些模块取代了基于字符串的提示技巧。为了取代提示黑客和一次性合成数据生成器，**DSPy** 还为您提供了通用优化器 (`BootstrapFewShotWithRandomSearch` 或 `MIPRO`)，这些是更新程序参数的算法。每当您修改代码、数据、断言或指标时，您可以重新 _编译_ 您的程序，**DSPy** 将创建适应您更改的新有效提示。