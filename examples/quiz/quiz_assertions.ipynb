{"cells":[{"cell_type":"markdown","metadata":{},"source":["<img src=\"../../docs/images/DSPy8.png\" alt=\"DSPy7 图片\" height=\"150\"/>\n","\n","## **DSPy 断言**: 对基础模型施加计算约束\n","\n","### **QuizGen**: 生成多项选择题问题"]},{"cell_type":"markdown","metadata":{},"source":["[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/quiz/quiz_assertions.ipynb)\n","\n","\n","这个笔记本突出了[**DSPy断言**](https://dspy-docs.vercel.app/docs/building-blocks/assertions)的一个示例，允许在DSPy程序中声明计算约束。\n","\n","\n","这个笔记本建立在**DSPy**框架的基本概念之上。跟随这个笔记本的先决条件是已经阅读了[DSPy教程](../../intro.ipynb)，[**DSPy断言文档**](https://dspy-docs.vercel.app/docs/building-blocks/assertions)以及LongFormQA中的DSPy断言入门教程(../longformqa/longformqa_assertions.ipynb)。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 克隆代码库\n","!git clone https://huggingface.co/arnavs11/DSPy_QuizGen_Cache\n","%cd DSPy_QuizGen_Cache/\n","!git checkout master\n","%cd ..\n","import os\n","repo_clone_path = '/content/DSPy_QuizGen_Cache'\n","\n","# 检查'/content'目录是否可写\n","if not os.access('/content', os.W_OK):\n","    # 如果'/content'目录不可写，选择另一个目录\n","    # 例如：使用相对于当前工作目录的目录\n","    repo_clone_path = os.path.join(os.getcwd(), 'DSPy_QuizGen_Cache')\n","\n","# 为此笔记本设置缓存\n","os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = repo_clone_path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","import sys\n","import os\n","import regex as re\n","import json\n","\n","try: # 当在谷歌Colab上时，让我们克隆笔记本以便下载缓存。\n","    import google.colab\n","    repo_path = 'dspy'\n","    \n","    !git -C $repo_path pull origin || git clone https://github.com/stanfordnlp/dspy $repo_path\n","except:\n","    repo_path = '.'\n","\n","if repo_path not in sys.path:\n","    sys.path.append(repo_path)\n","\n","\n","import pkg_resources # 如果尚未安装该软件包，则安装该软件包\n","if not \"dspy-ai\" in {pkg.key for pkg in pkg_resources.working_set}:\n","    !pip install -U pip\n","    !pip install dspy-ai\n","    !pip install openai~=0.28.1\n","    !pip install -e $repo_path\n","\n","import dspy\n","from dspy.predict import Retry\n","from dspy.datasets import HotPotQA\n","from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n","from dspy.evaluate.evaluate import Evaluate\n","from dspy.primitives.assertions import assert_transform_module, backtrack_handler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 创建一个 ColBERTv2 模型，连接到指定的 URL\n","colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n","\n","# 配置 DeepSpeed 设置，使用上面创建的 ColBERTv2 模型\n","dspy.settings.configure(rm=colbertv2_wiki17_abstracts)\n","\n","# 创建一个 OpenAI 模型，使用指定的 GPT-3.5 Turbo 模型和最大 token 数为 500\n","turbo = dspy.OpenAI(model='gpt-3.5-turbo-0613', max_tokens=500)\n","\n","# 配置 DeepSpeed 设置，使用上面创建的 OpenAI 模型，同时设置 trace 为空列表，温度为 0.7\n","dspy.settings.configure(lm=turbo, trace=[], temperature=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# 创建HotPotQA数据集对象，指定训练集种子为1，训练集大小为300，评估集种子为2023，开发集大小为300，测试集大小为0，保留详细信息\n","dataset = HotPotQA(train_seed=1, train_size=300, eval_seed=2023, dev_size=300, test_size=0, keep_details=True)\n","\n","# 从训练集中提取问题和答案，构建训练集\n","trainset = [x.with_inputs('question', 'answer') for x in dataset.train]\n","\n","# 从开发集中提取问题和答案，构建开发集\n","devset = [x.with_inputs('question', 'answer') for x in dataset.dev]"]},{"cell_type":"markdown","metadata":{},"source":["### 3] QuizGen\n","\n","让我们介绍一个新任务：QuizGen。\n","\n","QuizGen接收HotPotQA数据点，并将其转换为带有相应选项的多项选择测验问题。每个问题的选项集以JSON键值对格式生成。在这种情况下，我们指定生成4个选项。"]},{"cell_type":"markdown","metadata":{},"source":["通过这个程序，我们的目标是生成符合以下准则的测验选项：\n","1. 生成的选项以JSON格式呈现。\n","2. 生成的选项包括正确答案。\n","3. 生成的选项除了正确答案外还包括可信的干扰选项。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class GenerateAnswerChoices(dspy.Signature):\n","    \"\"\"生成包含指定问题的正确答案和可信干扰项的JSON格式答案选项。\"\"\"\n","    question = dspy.InputField()\n","    correct_answer = dspy.InputField()\n","    number_of_choices = dspy.InputField()\n","    answer_choices = dspy.OutputField(desc='JSON键值对')\n","\n","class QuizAnswerGenerator(dspy.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.generate_choices = dspy.ChainOfThought(GenerateAnswerChoices)\n","\n","    def forward(self, question, answer):\n","        choices = self.generate_choices(question=question, correct_answer=answer, number_of_choices=number_of_choices).answer_choices\n","        return dspy.Prediction(choices = choices)\n","\n","number_of_choices = '4'\n","quiz_generator = QuizAnswerGenerator()"]},{"cell_type":"markdown","metadata":{},"source":["### 4] 评估 - 内在和外在\n","\n","#### 内在度量：通过内部计算约束的目标\n","\n","**有效格式** - 输出的答案选择应该是JSON格式，经过解析键值对后进行验证。\n","\n","**正确答案包含** - 这是一个一般性检查，以确保生成的测验选择实际上包含问题的正确答案。\n","\n","**合理的干扰项** - 这个验证是为了检查生成的选择是否包含干扰项答案选项，这些选项是问题的合理答案。我们定义并调用另一个**DSPy**程序：``Predict``在``AssessQuizChoices``上，依赖于相同的LM来回答问题：`“答案选择中的干扰项是否合理，且不容易识别为不正确？”`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def format_checker(choice_string):\n","    # 检查输入的选项字符串是否符合要求\n","    try:\n","        choices = json.loads(choice_string)\n","        if isinstance(choices, dict) and all(isinstance(key, str) and isinstance(value, str) for key, value in choices.items()):\n","            return True\n","    except json.JSONDecodeError:\n","        return False\n","\n","    return False\n","\n","def is_correct_answer_included(correct_answer, generated_choices):\n","    # 检查正确答案是否包含在生成的选项中\n","    try:\n","        choices_dict = json.loads(generated_choices)\n","        return correct_answer in choices_dict.values()\n","    except json.JSONDecodeError:\n","        return False\n","\n","def is_plausibility_yes(assessment_answer):\n","    \"\"\"检查评估答案的第一个单词是否为'yes'。\"\"\"\n","    return assessment_answer.split()[0].lower() == 'yes'\n","    \n","class AssessQuizChoices(dspy.Signature):\n","    \"\"\"沿着指定维度评估测验答案选项的质量。\"\"\"\n","    \n","    question = dspy.InputField()\n","    answer_choices = dspy.InputField()\n","    assessment_question = dspy.InputField()\n","    assessment_answer = dspy.OutputField(desc=\"是或否\")\n","\n","def format_valid_metric(gold, pred, trace=None):\n","    generated_choices = pred.choices\n","    format_valid = format_checker(generated_choices)\n","    score = format_valid\n","    return score\n","\n","def is_correct_metric(gold, pred, trace=None):\n","    correct_answer, generated_choices = gold.answer, pred.choices\n","    correct_included = is_correct_answer_included(correct_answer, generated_choices)\n","    score = correct_included\n","    return score\n","\n","def plausibility_metric(gold, pred, trace=None):\n","    question, generated_choices = gold.question, pred.choices\n","    plausibility_question = \"答案选项中的干扰项是否合理且不容易识别为不正确？\"\n","    plausibility_assessment = dspy.Predict(AssessQuizChoices)(question=question, answer_choices=generated_choices, assessment_question=plausibility_question)\n","    plausibility_result = plausibility_assessment.assessment_answer.split()[0].lower() == 'yes'\n","    score = plausibility_result\n","    return score"]},{"cell_type":"markdown","metadata":{},"source":["#### 外部度量：评估生成的输出在下游任务中的整体质量和有效性\n","\n","外部度量被定义为生成的测验选项的整体质量，并通过一个综合度量进行评估，考虑了这些约束条件。\n","\n","综合度量保持了生成有效的测验选项所需的核心内在度量，验证有效的格式和正确答案的包含，并且整体综合度量返回了三个内在度量的平均分数。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def overall_metric(gold, pred, trace=None):\n","    # 从gold中获取问题、正确答案和生成的选项\n","    question, correct_answer, generated_choices = gold.question, gold.answer, pred.choices\n","    # 检查生成的选项的格式是否有效\n","    format_valid = format_checker(generated_choices)\n","    # 检查正确答案是否包含在生成的选项中\n","    correct_included = is_correct_answer_included(correct_answer, generated_choices)\n","    # 判断问题：生成的干扰项是否合理且不易识别为不正确？\n","    plausibility_question = \"Are the distractors in the answer choices plausible and not easily identifiable as incorrect?\"\n","    # 进行可信度评估\n","    plausibility_assessment = dspy.Predict(AssessQuizChoices)(question=question, answer_choices=generated_choices, assessment_question=plausibility_question)\n","    # 判断可信度评估结果是否为\"yes\"\n","    plausibility_result = plausibility_assessment.assessment_answer.split()[0].lower() == 'yes'\n","    # 计算得分，如果正确答案包含在生成的选项中且格式有效，则计算得分，否则得分为0\n","    score = (format_valid + correct_included + plausibility_result) / 3.0 if correct_included and format_valid else 0\n","    return score"]},{"cell_type":"markdown","metadata":{},"source":["因此，我们将评估定义如下："]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 定义评估指标列表\n","metrics = [format_valid_metric, is_correct_metric, plausibility_metric, overall_metric]\n","\n","# 遍历评估指标列表\n","for metric in metrics:\n","    # 创建 Evaluate 实例，传入评估指标、开发集、线程数、显示进度和显示表格参数\n","    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n","    # 使用评估器对 quiz_generator 进行评估\n","    evaluate(quiz_generator)"]},{"cell_type":"markdown","metadata":{},"source":["让我们来看一个示例测验选项生成："]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 选择第67个样本作为示例\n","example = devset[67]\n","# 使用问题生成器生成问题选项\n","quiz_choices = quiz_generator(question=example.question, answer=example.answer)\n","# 打印生成的问题选项\n","print(f'生成的问题选项: ', quiz_choices.choices)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 对于每一个指标，创建一个Evaluate对象，并传入相应参数\n","for metric in metrics:\n","    evaluate = Evaluate(metric=metric, devset=devset[67:68], num_threads=1, display_progress=True, display_table=5)\n","    # 调用evaluate对象，传入quiz_generator参数\n","    evaluate(quiz_generator)"]},{"cell_type":"markdown","metadata":{},"source":["我们看到生成的测验选项没有保持有效的 JSON 格式，这违反了有效格式和正确性检查，尽管这些选项被认为是合理的。我们还看到正确答案也被标记为“(正确答案)”，这并不是产生良好测验问题答案选择的意图。\n","\n","让我们看看如何集成 DSPy 断言并施加约束以产生更好的答案选择。"]},{"cell_type":"markdown","metadata":{},"source":["### 5] 引入断言：使用断言生成答题器\n","\n","让我们在 DSPy 断言语义中包含简单重申我们的计算约束的断言。\n","\n","在第一个**断言**中，我们检查生成的测验选择是否为 JSON 格式，如果不是，则断言：**\"答案选择的格式应为 JSON 格式。请相应进行修订。\"**\n","\n","我们还检查测验选择集合是否包含正确答案，并在违反时确保这一点，并提供反馈信息：**\"答案选择不包括问题的正确答案。请相应进行修订。\"**\n","\n","最后，我们评估可信的干扰选择是否确实是良好的干扰选项，如果不是，则断言：**\"答案选择不是可信的干扰选项，或者太容易被识别为不正确。请进行修订，提供更具挑战性和可信度的干扰选项。\"**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class QuizAnswerGeneratorWithAssertions(dspy.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.generate_choices = dspy.ChainOfThought(GenerateAnswerChoices)\n","\n","    def forward(self, question, answer):\n","        # 生成备选答案选项字符串\n","        choice_string = self.generate_choices(question=question, correct_answer=answer, number_of_choices=number_of_choices).answer_choices\n","        # 检查答案选项的格式是否为JSON格式\n","        dspy.Suggest(format_checker(choice_string), \"答案选项的格式应为JSON格式。请相应修改。\", target_module=GenerateAnswerChoices)\n","        # 检查答案选项是否包含正确答案\n","        dspy.Suggest(is_correct_answer_included(answer, choice_string), \"答案选项不包含问题的正确答案。请相应修改。\", target_module=GenerateAnswerChoices)\n","        # 判断是否干扰项在答案选项中是否合理且不易被识别为错误\n","        plausibility_question = \"干扰项在答案选项中是否合理且不易被识别为错误？\"\n","        plausibility_assessment = dspy.Predict(AssessQuizChoices)(question=question, answer_choices=choice_string, assessment_question=plausibility_question)\n","        # 判断干扰项是否合理\n","        dspy.Suggest(is_plausibility_yes(plausibility_assessment.assessment_answer), \"答案选项中的干扰项不合理或太容易被识别为错误。请修改以提供更具挑战性和合理性的干扰项。\", target_module=GenerateAnswerChoices)\n","        return dspy.Prediction(choices = choice_string)\n","\n","number_of_choices = '4'\n","# 断言转换模块\n","quiz_generator_with_assertions = assert_transform_module(QuizAnswerGeneratorWithAssertions().map_named_predictors(Retry), backtrack_handler)"]},{"cell_type":"markdown","metadata":{},"source":["让我们现在在开发集上评估`QuizAnswerGeneratorWithAssertions`。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 定义评估指标列表\n","metrics = [format_valid_metric, is_correct_metric, plausibility_metric, overall_metric]\n","\n","# 遍历评估指标列表\n","for metric in metrics:\n","    # 创建 Evaluate 对象，传入指标、开发集、线程数、显示进度和显示表格参数\n","    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n","    # 使用评估对象对 quiz_generator_with_assertions 进行评估\n","    evaluate(quiz_generator_with_assertions)"]},{"cell_type":"markdown","metadata":{},"source":["现在让我们看看随着断言的添加，我们生成的测验选项集是如何改进的。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 选择第67个样本作为示例\n","example = devset[67]\n","# 使用带有断言的quiz_generator生成问题和答案选项\n","quiz_choices = quiz_generator_with_assertions(question=example.question, answer=example.answer)\n","# 打印生成的测验选项\n","print(f'生成的测验选项: ', quiz_choices.choices)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 对于每个指标，创建一个Evaluate对象并进行评估\n","for metric in metrics:\n","    evaluate = Evaluate(metric=metric, devset=devset[67:68], num_threads=1, display_progress=True, display_table=30)\n","    # 使用quiz_generator_with_assertions对evaluate对象进行评估\n","    evaluate(quiz_generator_with_assertions)"]},{"cell_type":"markdown","metadata":{},"source":["我们看到，测验选项符合我们所有的约束条件！\n","\n","不仅所有答案选项都是合理的，并且已经移除了任何指示正确答案的标志，而且答案选项现在保持了有效的JSON格式，提供了4个可能的答案选项，其中包括正确答案。"]},{"cell_type":"markdown","metadata":{},"source":["### 6] 使用断言进行编译\n","\n","我们可以利用 **DSPy** 的 `BootstrapFewShotWithRandomSearch` 优化器，自动生成少样本演示，并对候选进行随机搜索，以输出最佳编译程序。我们通过 `final_metric` 综合指标来评估这一过程。\n","\n","我们可以首先在 `QuizAnswerGenerator` 上进行评估，看看在不包含断言的情况下编译的表现如何。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 创建一个BootstrapFewShotWithRandomSearch对象，指定评估指标为overall_metric，最大bootstrap演示次数为2，候选程序数量为6\n","teleprompter = BootstrapFewShotWithRandomSearch(metric=overall_metric, max_bootstrapped_demos=2, num_candidate_programs=6)\n","# 编译quiz_generator生成的学生和老师模型，使用trainset进行训练，使用devset的前100个样本进行验证\n","compiled_quiz_generator = teleprompter.compile(student=quiz_generator, teacher=quiz_generator, trainset=trainset, valset=devset[:100])\n","\n","# 遍历metrics列表中的每个评估指标\n","for metric in metrics:\n","    # 创建一个Evaluate对象，指定评估指标为当前metric，使用devset进行评估，线程数为1，显示进度，每次显示前5个结果\n","    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n","    # 对编译后的quiz_generator模型进行评估\n","    evaluate(compiled_quiz_generator)"]},{"cell_type":"markdown","metadata":{},"source":["现在我们在两种设置下测试编译与断言：\n","\n","**带断言的编译**：在编译过程中进行基于断言的示例引导和反例引导。教师拥有断言，而学生没有，因为学生从教师的基于断言的引导示例中学习。\n","\n","**带断言的编译+推理**：为教师和学生提供基于断言的优化，以在编译和推理过程中提供增强的基于断言的输出。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 创建一个BootstrapFewShotWithRandomSearch对象，指定评估指标为overall_metric，最大bootstrap演示次数为2，候选程序数量为6\n","teleprompter = BootstrapFewShotWithRandomSearch(metric=overall_metric, max_bootstrapped_demos=2, num_candidate_programs=6)\n","# 编译带有断言的测验生成器，使用teleprompter对象进行编译，指定学生为quiz_generator，老师为quiz_generator_with_assertions，训练集为trainset，验证集为devset的前100个样本\n","compiled_with_assertions_quiz_generator = teleprompter.compile(student=quiz_generator, teacher=quiz_generator_with_assertions, trainset=trainset, valset=devset[:100])\n","\n","# 遍历metrics列表中的每个评估指标\n","for metric in metrics:\n","    # 创建一个Evaluate对象，指定评估指标为当前metric，验证集为devset，线程数为1，显示进度为True，显示表格为5\n","    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n","    # 对编译后的带有断言的测验生成器进行评估\n","    evaluate(compiled_with_assertions_quiz_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 创建一个BootstrapFewShotWithRandomSearch对象，使用overall_metric作为度量标准，最大引导演示次数为2，候选程序数量为6\n","teleprompter = BootstrapFewShotWithRandomSearch(metric=overall_metric, max_bootstrapped_demos=2, num_candidate_programs=6)\n","# 编译带有断言的测验生成器，使用teleprompter作为引导器，学生和老师都是quiz_generator_with_assertions，训练集为trainset，验证集为devset的前100个样本\n","compiled_quiz_generator_with_assertions = teleprompter.compile(student=quiz_generator_with_assertions, teacher=quiz_generator_with_assertions, trainset=trainset, valset=devset[:100])\n","\n","# 遍历metrics列表\n","for metric in metrics:\n","    # 创建一个Evaluate对象，使用指定的度量标准metric，验证集为devset，线程数为1，显示进度条，每5个显示一次表格\n","    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n","    # 对编译后的带有断言的测验生成器进行评估\n","    evaluate(compiled_quiz_generator_with_assertions)"]}],"metadata":{"kernelspec":{"display_name":"dspy_dev","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
