{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","<img src=\"../../docs/images/DSPy8.png\" alt=\"DSPy7 Image\" height=\"150\"/>\n","\n","## **DSPy断言**: 对基础模型施加计算约束\n","\n","### **TweetGen**: 生成推文以回答问题"]},{"cell_type":"markdown","metadata":{},"source":["[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/tweets/tweets_assertions.ipynb)\n","\n","\n","这个笔记本突出了[**DSPy Assertions**](https://dspy-docs.vercel.app/docs/building-blocks/assertions)的一个示例，允许在DSPy程序中声明计算约束。\n","\n","\n","这个笔记本建立在**DSPy**框架的基本概念之上。在阅读本笔记本之前，需要先完成[DSPy教程](../../intro.ipynb)、[**DSPy Assertions文档**](https://dspy-docs.vercel.app/docs/building-blocks/assertions)以及LongFormQA中的DSPy Assertions入门教程。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 克隆指定的git仓库\n","!git clone https://huggingface.co/arnavs11/DSPy_TweetGen_Cache\n","%cd DSPy_TweetGen_Cache/\n","# 切换到master分支\n","!git checkout master\n","%cd ..\n","import os\n","repo_clone_path = '/content/DSPy_TweetGen_Cache'\n","\n","# 检查'/content'目录是否可写\n","if not os.access('/content', os.W_OK):\n","    # 如果'/content'目录不可写，选择一个替代目录\n","    # 例如：使用相对于当前工作目录的目录\n","    repo_clone_path = os.path.join(os.getcwd(), 'DSPy_TweetGen_Cache')\n","\n","# 为此笔记本设置缓存\n","os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = repo_clone_path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","import sys\n","import os\n","import regex as re\n","import json\n","\n","try: # 当在谷歌Colab上时，让我们克隆笔记本以便下载缓存。\n","    import google.colab\n","    repo_path = 'dspy'\n","    \n","    !git -C $repo_path pull origin || git clone https://github.com/stanfordnlp/dspy $repo_path\n","except:\n","    repo_path = '.'\n","\n","if repo_path not in sys.path:\n","    sys.path.append(repo_path)\n","\n","\n","import pkg_resources # 如果尚未安装该软件包，请安装该软件包\n","if not \"dspy-ai\" in {pkg.key for pkg in pkg_resources.working_set}:\n","    !pip install -U pip\n","    !pip install dspy-ai\n","    !pip install openai~=0.28.1\n","    !pip install -e $repo_path\n","\n","import dspy\n","from dspy.predict import Retry\n","from dspy.datasets import HotPotQA\n","from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n","from dsp.utils import deduplicate\n","from dspy.evaluate.evaluate import Evaluate\n","from dspy.primitives.assertions import assert_transform_module, backtrack_handler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 创建一个 ColBERTv2 模型，使用给定的 URL 连接到 wiki17_abstracts 数据集\n","colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n","\n","# 配置默认设置，将 ColBERTv2 模型设置为默认的检索模型\n","dspy.settings.configure(rm=colbertv2_wiki17_abstracts)\n","\n","# 创建一个 OpenAI 模型，使用 'gpt-3.5-turbo-0613' 模型和最大 token 数为 500\n","turbo = dspy.OpenAI(model='gpt-3.5-turbo-0613', max_tokens=500)\n","\n","# 配置默认设置，将 OpenAI 模型设置为默认的语言模型，同时设置 trace 为空列表，温度为 0.7\n","dspy.settings.configure(lm=turbo, trace=[], temperature=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 导入HotPotQA数据集\n","dataset = HotPotQA(train_seed=1, train_size=300, eval_seed=2023, dev_size=300, test_size=0, keep_details=True)\n","\n","# 从训练集中提取问题和答案作为输入\n","trainset = [x.with_inputs('question', 'answer') for x in dataset.train]\n","\n","# 从验证集中提取问题和答案作为输入\n","devset = [x.with_inputs('question', 'answer') for x in dataset.dev]"]},{"cell_type":"markdown","metadata":{},"source":["### 3] TweetGen\n","\n","让我们介绍一个新任务：TweetGen。我们扩展了`Multi-Hop QA`程序，但现在的目标是以推文的形式呈现答案生成。\n","\n","`Tweeter`模块捕获了从`Multi-Hop QA`中的查询生成、段落检索和上下文组装中的迭代多跳生成过程。`GenerateTweet`层现在利用上下文和问题一起生成一个有效回答问题的推文。"]},{"cell_type":"markdown","metadata":{},"source":["通过这个程序，我们的目标是生成符合以下准则的推文：\n","1. 推文没有标签。\n","2. 推文包含正确答案\n","3. 推文在字符限制内。\n","4. 推文具有吸引力\n","5. 推文忠实"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class GenerateSearchQuery(dspy.Signature):\n","    \"\"\"编写一个简单的搜索查询，以帮助回答一个复杂的问题。\"\"\"\n","    context = dspy.InputField(desc=\"可能包含相关事实\")\n","    question = dspy.InputField()\n","    query = dspy.OutputField()\n","\n","class GenerateTweet(dspy.Signature):\n","    \"\"\"生成一个引人入胜的推文，有效地回答一个问题，忠实于上下文，不超过280个字符，并且没有标签。\"\"\"\n","    question = dspy.InputField()\n","    context = dspy.InputField(desc=\"可能包含相关事实\")\n","    tweet = dspy.OutputField()\n","\n","class Tweeter(dspy.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.generate_tweet = dspy.ChainOfThought(GenerateTweet)\n","\n","    def forward(self, question, answer):\n","        context = []\n","        max_hops=2\n","        passages_per_hop=3\n","        generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n","        retrieve = dspy.Retrieve(k=passages_per_hop)\n","        for hop in range(max_hops):\n","            query = generate_query[hop](context=context, question=question).query\n","            passages = retrieve(query).passages\n","            context = deduplicate(context + passages)  # 去重\n","        generated_tweet = self.generate_tweet(question=question, context=context).tweet\n","        return dspy.Prediction(generated_tweet=generated_tweet, context=context)\n","    \n","tweeter = Tweeter()"]},{"cell_type":"markdown","metadata":{},"source":["### 4] 评估 - 内在和外在\n","\n","#### 内在度量：通过内部计算约束是目标\n","\n","**无标签** - 这是一个用户个性化的约束，用于测试模型能否遵循一个特定但简单的指导方针，即在生成的推文中不包含任何标签。\n","\n","**正确答案包含** - 这是一个通用检查，以确保推文确实包含问题的正确答案。\n","\n","**长度内** - 此检查遵循 Twitter 平台的指导方针，每条推文限制为 280 个字符。\n","\n","**参与度** - 为了验证推文的参与质量，我们定义并调用另一个 **DSPy** 程序：``Predict`` 在 ``AssessTweet`` 上，依赖于相同的 LM 来回答问题：`“评估的文本是否构成一个独立的、引人入胜的推文？如果不引人入胜，请说不。”`\n","\n","**忠实度** - 为了验证推文对其引用上下文的忠实度，我们同样使用上面的 `AssessTweet`，但用以下问题提示它：`“评估的文本是否基于上下文？如果包含不在上下文中的重要事实，请说不。”`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","from daisy import dspy\n","\n","def has_no_hashtags(text):\n","    # 检查文本中是否包含hashtags\n","    return len(re.findall(r\"#\\w+\", text)) == 0\n","\n","def is_within_length_limit(text, length_limit=280):\n","    # 检查文本长度是否在限制范围内\n","    return len(text) <= length_limit\n","\n","def is_assessment_yes(assessment_answer):\n","    \"\"\"检查评估答案的第一个单词是否为'yes'。\"\"\"\n","    return assessment_answer.split()[0].lower() == 'yes'\n","\n","def has_correct_answer(text, answer):\n","    # 检查文本中是否包含正确答案\n","    return answer in text\n","\n","class AssessTweet(dspy.Signature):\n","    \"\"\"评估推文在指定维度上的质量。\"\"\"\n","\n","    context = dspy.InputField(desc='如果不适用，请忽略')\n","    assessed_text = dspy.InputField()\n","    assessment_question = dspy.InputField()\n","    assessment_answer = dspy.OutputField(desc=\"是或否\")\n","\n","def no_hashtags_metric(gold, pred, trace=None):\n","    tweet = pred.generated_tweet\n","    no_hashtags = has_no_hashtags(tweet)\n","    score = no_hashtags\n","    return score\n","\n","def is_correct_metric(gold, pred, trace=None):\n","    answer, tweet = gold.answer, pred.generated_tweet\n","    correct = has_correct_answer(tweet, answer)\n","    score = correct\n","    return score\n","\n","def within_length_metric(gold, pred, trace=None):\n","    tweet = pred.generated_tweet\n","    within_length_limit = is_within_length_limit(tweet, 280)\n","    score = within_length_limit\n","    return score\n","\n","def engaging_metric(gold, pred, trace=None):\n","    tweet = pred.generated_tweet\n","    engaging = \"评估文本是否构成一个自包含、引人入胜的推文？如果不引人入胜，请说不。\"\n","    engaging = dspy.Predict(AssessTweet)(context='N/A', assessed_text=tweet, assessment_question=engaging)\n","    engaging = engaging.assessment_answer.split()[0].lower() == 'yes'\n","    score = engaging\n","    return score\n","\n","def faithful_metric(gold, pred, trace=None):\n","    context, tweet = pred.context, pred.generated_tweet\n","    faithful = \"评估文本是否基于上下文？如果包含重要事实而上下文中没有，请说不。\"   \n","    faithful = dspy.Predict(AssessTweet)(context=context, assessed_text=tweet, assessment_question=faithful)\n","    faithful = faithful.assessment_answer.split()[0].lower() == 'yes'\n","    score = faithful\n","    return score"]},{"cell_type":"markdown","metadata":{},"source":["#### 外部度量：评估生成输出在下游任务中的整体质量和有效性\n","\n","外部度量被定义为生成推文在遵循所述约束条件方面的整体质量，并且这是通过一个综合度量来评估的。\n","\n","在保持形成有效推文的最相关的内在度量（正确性和长度约束）的同时，整体综合度量返回5个内在度量的平均分数。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def overall_metric(gold, pred, trace=None):\n","    # 提取gold和pred中的信息\n","    answer, context, tweet = gold.answer, pred.context, pred.generated_tweet\n","    # 检查tweet中是否有hashtags\n","    no_hashtags = has_no_hashtags(tweet)\n","    # 检查tweet是否在280个字符以内\n","    within_length_limit = is_within_length_limit(tweet, 280)\n","    # 检查tweet是否包含正确答案\n","    correct = has_correct_answer(tweet, answer)\n","    # 评估tweet是否构成一个自包含且引人入胜的推文\n","    engaging = \"Does the assessed text make for a self-contained, engaging tweet? Say no if it is not engaging.\"\n","    # 评估tweet是否基于上下文\n","    faithful = \"Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\"   \n","    # 使用dspy.Predict对tweet进行faithful和engaging的评估\n","    faithful = dspy.Predict(AssessTweet)(context=context, assessed_text=tweet, assessment_question=faithful)\n","    engaging = dspy.Predict(AssessTweet)(context='N/A', assessed_text=tweet, assessment_question=engaging)\n","    # 将评估结果转换为布尔值\n","    engaging, faithful = [m.assessment_answer.split()[0].lower() == 'yes' for m in [engaging, faithful]]\n","    # 计算得分\n","    score = (correct + engaging + faithful + no_hashtags + within_length_limit) if correct and within_length_limit else 0\n","    return score / 5.0"]},{"cell_type":"markdown","metadata":{},"source":["因此，我们将评估定义如下："]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 定义评估指标列表\n","metrics = [no_hashtags_metric, is_correct_metric, within_length_metric, engaging_metric, faithful_metric, overall_metric]\n","\n","# 遍历评估指标列表\n","for metric in metrics:\n","    # 创建 Evaluate 实例\n","    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n","    # 对 tweeter 进行评估\n","    evaluate(tweeter)"]},{"cell_type":"markdown","metadata":{},"source":["让我们来看一个生成推文的示例："]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["example = devset[118]  # 从devset中获取第118个样本\n","tweet = tweeter(question=example.question, answer=example.answer)  # 使用example的问题和答案创建一个tweet对象\n","print(f'生成的推文: ', tweet.generated_tweet)  # 打印生成的推文\n","tweet.context  # 输出tweet的上下文"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for metric in metrics:  # 遍历metrics列表中的每一个指标\n","    evaluate = Evaluate(metric=metric, devset=devset[118:119], num_threads=1, display_progress=True, display_table=5)  # 创建一个Evaluate对象，传入指标、数据集、线程数、显示进度和显示表格参数\n","    evaluate(tweeter)  # 对tweeter进行评估"]},{"cell_type":"markdown","metadata":{},"source":["在这个例子中，我们看到生成的推文长度为151个字符，位于280个字符的范围内。实际上，它包含了正确答案`Hooke`。\n","\n","然而，它未能包含标签，我们在推文末尾看到了`#knowledge`。此外，这条推文被确定为缺乏吸引力，从视觉测试来看，它只是简单地陈述了答案而已。\n","\n","让我们尝试修复这个问题，并使用DSPy断言生成推文。"]},{"cell_type":"markdown","metadata":{},"source":["### 5] 引入断言：带有断言的 TweeterWithAssertions\n","\n","为了纠正这些不同的错误，让我们在 DSPy 断言语义中包含简单重申我们的计算约束的断言。\n","\n","在第一个**断言**中，我们通过正则表达式检查生成的推文是否有任何标签，并且如果违反了，断言：**\"请修改推文以删除其后的标签短语。\"**\n","\n","类似地，我们检查推文的长度，如果不在 280 个字符内，我们发送反馈信息：**\"请确保推文在 {280} 个字符内。\"**\n","\n","我们检查生成的推文是否包含答案，如果没有，我们断言：**\"推文不包括问题的正确答案。请相应修改。\"**\n","\n","对于参与度和忠实度检查，我们利用上面的设置，检查相应评估是否确定为`是`或`否`。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TweeterWithAssertions(dspy.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.generate_tweet = dspy.ChainOfThought(GenerateTweet)\n","\n","    def forward(self, question, answer):\n","        context = []\n","        max_hops = 2\n","        passages_per_hop = 3\n","        generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n","        retrieve = dspy.Retrieve(k=passages_per_hop)\n","        for hop in range(max_hops):\n","            # 生成查询语句\n","            query = generate_query[hop](context=context, question=question).query\n","            # 检索相关段落\n","            passages = retrieve(query).passages\n","            # 去重并更新上下文\n","            context = deduplicate(context + passages)\n","        # 生成推文\n","        generated_tweet = self.generate_tweet(question=question, context=context).tweet\n","        # 检查推文是否包含标签短语\n","        dspy.Suggest(has_no_hashtags(generated_tweet), f\"请修改推文以删除后面的标签短语。\", target_module=GenerateTweet)\n","        # 检查推文长度是否符合限制\n","        dspy.Suggest(is_within_length_limit(generated_tweet, 280), f\"请确保推文在{280}个字符以内。\", target_module=GenerateTweet)\n","        # 检查推文是否包含正确答案\n","        dspy.Suggest(has_correct_answer(generated_tweet, answer), \"推文未包含正确答案。请相应修改。\", target_module=GenerateTweet)\n","        engaging_question = \"评估的文本是否构成一个自成一体、引人入胜的推文？如果不引人入胜，请选择否。\"\n","        engaging_assessment = dspy.Predict(AssessTweet)(context=context, assessed_text=generated_tweet, assessment_question=engaging_question)\n","        # 检查推文是否引人入胜\n","        dspy.Suggest(is_assessment_yes(engaging_assessment.assessment_answer), \"文本不够引人入胜。请修改以增加吸引力。\", target_module=GenerateTweet)\n","        faithful_question = \"评估的文本是否基于上下文？如果包含重要事实而上下文中没有，请选择否。\"\n","        faithful_assessment = dspy.Predict(AssessTweet)(context='N/A', assessed_text=generated_tweet, assessment_question=faithful_question)\n","        # 检查推文是否基于上下文\n","        dspy.Suggest(is_assessment_yes(faithful_assessment.assessment_answer), \"文本包含不忠实的元素或上下文中没有的重要事实。请修改以提高准确性。\", target_module=GenerateTweet)\n","        return dspy.Prediction(generated_tweet=generated_tweet, context=context)\n","\n","tweeter_with_assertions = assert_transform_module(TweeterWithAssertions().map_named_predictors(Retry), backtrack_handler)"]},{"cell_type":"markdown","metadata":{},"source":["让我们现在在开发集上评估`TweeterWithAssertions`。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 定义评估指标列表\n","metrics = [no_hashtags_metric, is_correct_metric, within_length_metric, engaging_metric, faithful_metric, overall_metric]\n","\n","# 遍历每个评估指标\n","for metric in metrics:\n","    # 创建 Evaluate 实例，传入评估指标、开发集、线程数、显示进度和显示表格\n","    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n","    # 对 tweeter_with_assertions 进行评估\n","    evaluate(tweeter_with_assertions)"]},{"cell_type":"markdown","metadata":{},"source":["现在让我们看一下，通过添加断言，我们生成的推文如何得到改进。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["example = devset[118]  # 从devset中获取第118个示例\n","tweet = tweeter_with_assertions(question=example.question, answer=example.answer)  # 使用示例的问题和答案创建一个tweet对象\n","print(f'生成的推文：', tweet.generated_tweet)  # 打印生成的推文\n","tweet.context  # 输出tweet的上下文"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 对于每一个指标，创建一个Evaluate对象并进行评估\n","for metric in metrics:\n","    # 创建Evaluate对象，传入指标、开发集的子集、线程数、显示进度和显示表格\n","    evaluate = Evaluate(metric=metric, devset=devset[118:119], num_threads=1, display_progress=True, display_table=5)\n","    # 对tweeter_with_assertions进行评估\n","    evaluate(tweeter_with_assertions)"]},{"cell_type":"markdown","metadata":{},"source":["我们看到推文在遵循我们设定的所有限制条件后有了显著的改进！\n","\n","它不再有标签，既引人入胜又忠实，同时在280个字符内包含了正确答案。令人兴奋！"]},{"cell_type":"markdown","metadata":{},"source":["### 6] 使用断言进行编译\n","\n","我们可以利用 **DSPy** 的 `BootstrapFewShotWithRandomSearch` 优化器，自动生成少样本演示，并对候选进行随机搜索，输出最佳编译程序。我们通过 `overall_metric` 综合指标来评估这一过程。\n","\n","我们可以首先在 `Tweeter` 上进行评估，看看在不包含断言的情况下编译的表现如何。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 创建一个BootstrapFewShotWithRandomSearch对象，使用overall_metric作为度量标准，最大bootstrapped示例数为2，候选程序数为6\n","teleprompter = BootstrapFewShotWithRandomSearch(metric=overall_metric, max_bootstrapped_demos=2, num_candidate_programs=6)\n","# 编译teleprompter，将学生模型tweeter作为学生和老师，使用trainset进行训练，使用devset的前100个示例进行验证\n","compiled_tweeter = teleprompter.compile(student=tweeter, teacher=tweeter, trainset=trainset, valset=devset[:100])\n","\n","# 遍历metrics列表\n","for metric in metrics:\n","    # 创建一个Evaluate对象，使用指定的metric作为度量标准，devset作为验证集，使用1个线程，显示进度，每5个显示一次表格\n","    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n","    # 对编译后的tweeter模型进行评估\n","    evaluate(compiled_tweeter)"]},{"cell_type":"markdown","metadata":{},"source":["现在我们在两种设置下进行带有断言的编译测试：\n","\n","**带有断言的编译**：在编译过程中进行基于断言的示例引导和反例引导。老师拥有断言，而学生没有，学生从老师的基于断言的引导示例中学习。\n","\n","**带有断言的编译 + 推理**：为老师和学生提供基于断言的优化，以在编译和推理过程中提供增强的基于断言的输出。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 创建一个BootstrapFewShotWithRandomSearch对象，使用overall_metric作为度量标准，最大引导演示次数为2，候选程序数量为6\n","teleprompter = BootstrapFewShotWithRandomSearch(metric=overall_metric, max_bootstrapped_demos=2, num_candidate_programs=6)\n","# 编译teleprompter对象，使用tweeter作为学生模型，tweeter_with_assertions作为教师模型，trainset作为训练集，devset的前100个样本作为验证集\n","compiled_with_assertions_tweeter = teleprompter.compile(student=tweeter, teacher=tweeter_with_assertions, trainset=trainset, valset=devset[:100])\n","\n","# 遍历metrics列表中的每个度量标准\n","for metric in metrics:\n","    # 创建一个Evaluate对象，使用当前遍历到的metric作为度量标准，devset作为验证集，使用1个线程，显示进度，每次显示5个结果\n","    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n","    # 对compiled_with_assertions_tweeter对象进行评估\n","    evaluate(compiled_with_assertions_tweeter)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 创建一个BootstrapFewShotWithRandomSearch对象，指定metric为overall_metric，最大bootstrapped示例数为2，候选程序数为6，线程数为1\n","teleprompter = BootstrapFewShotWithRandomSearch(metric=overall_metric, max_bootstrapped_demos=2, num_candidate_programs=6, num_threads=1)\n","# 编译tweeter_with_assertions模型，作为学生和教师模型，使用trainset进行训练，使用devset的前100个样本进行验证\n","compiled_tweeter_with_assertions = teleprompter.compile(student=tweeter_with_assertions, teacher=tweeter_with_assertions, trainset=trainset, valset=devset[:100])\n","\n","# 遍历metrics列表\n","for metric in metrics:\n","    # 创建一个Evaluate对象，指定metric为当前遍历到的metric，devset为devset，线程数为1，显示进度，显示前5个结果\n","    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n","    # 对编译后的tweeter_with_assertions模型进行评估\n","    evaluate(compiled_tweeter_with_assertions)"]}],"metadata":{"kernelspec":{"display_name":"dspy_dev","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
